{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Notes before running code:\n",
        "\n",
        "\n",
        "*   Make sure to start the session by going to edit -> notebook settings -> and select the T4 GPU as the hardware accelerator.\n",
        "*    When you run the installs, a warning window will pop up telling you to restart your session. Wait until the cell is done running before accepting.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9WmE6p6dGSGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZU4hdh4J8zZp"
      },
      "outputs": [],
      "source": [
        "!pip install -U keras keras-nlp tensorflow datasets\n",
        "!pip install -q bertopic sentence-transformers umap-learn hdbscan==0.8.33"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U numpy\n",
        "!pip install --force-reinstall hdbscan"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xvPWd_Dk81Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = load_dataset(\"SetFit/bbc-news\")\n",
        "df = pd.DataFrame(dataset[\"train\"])\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "XWU2O3uQ9BqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BERTopic"
      ],
      "metadata": {
        "id": "v8m1vHjuMq6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan\n",
        "from bertopic import BERTopic\n",
        "\n",
        "df.columns\n",
        "docs = df[\"text\"].tolist()\n",
        "\n",
        "hdbscan_model = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=70,      # bigger = fewer topics\n",
        "    min_samples=2,            # bigger = more points classified as noise\n",
        "    metric=\"euclidean\",\n",
        "    cluster_selection_method=\"eom\"\n",
        ")\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=\"all-MiniLM-L6-v2\",\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ],
      "metadata": {
        "id": "a3XPQ0ks9IKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"bertopic_topic\"] = topics\n",
        "pd.crosstab(df[\"label_text\"], df[\"bertopic_topic\"])"
      ],
      "metadata": {
        "id": "pr02VWvn9bQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tune an LLM for topic classification**"
      ],
      "metadata": {
        "id": "VsxgVuFuB8sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import openpyxl\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "TJiFuSs9CGj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "num_classes = len(df[\"label_text\"].value_counts())\n",
        "\n",
        "colors = plt.cm.Dark2(np.linspace(0, 1, num_classes))\n",
        "iter_color = iter(colors)\n",
        "\n",
        "df['label_text'].value_counts().plot.barh(title=\"Topic (n, %)\",\n",
        "                                                 ylabel=\"Topic Name\",\n",
        "                                                 color=colors,\n",
        "                                                 figsize=(9,9))\n",
        "\n",
        "for i, v in enumerate(df['label_text'].value_counts()):\n",
        "  c = next(iter_color)\n",
        "  plt.text(v, i,\n",
        "           \" \"+str(v)+\", \"+str(round(v*100/df.shape[0],2))+\"%\",\n",
        "           color=c,\n",
        "           va='center',\n",
        "           fontweight='bold')"
      ],
      "metadata": {
        "id": "rLMnl9XkCJWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras_nlp\n",
        "# Add an index column to track original positions\n",
        "df[\"original_index\"] = df.index\n",
        "y = tf.keras.utils.to_categorical(df[\"label\"].values, num_classes=5)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.25, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "JEbBgJgkCLZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 512     # Sets the maximum number of tokens per input sequence\n",
        "\n",
        "# Preprocessor (Creates text preprocessing pipeline)\n",
        "bert_preprocess = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_base_en_uncased\",\n",
        "    sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "# encoder (brains of BERT)\n",
        "bert_encoder = keras_nlp.models.BertBackbone.from_preset(\n",
        "    \"bert_base_en_uncased\")\n",
        "\n",
        "\n",
        "# Model inputs\n",
        "input_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\")\n",
        "padding_mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32, name=\"padding_mask\")\n",
        "segment_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "bert_encoder.trainable = False    # Specifies that the bert encoder is frozen and not fine tuned\n",
        "\n",
        "x = bert_encoder({\n",
        "    \"token_ids\": input_ids,\n",
        "    \"padding_mask\": padding_mask,\n",
        "    \"segment_ids\": segment_ids\n",
        "})[\"pooled_output\"]\n",
        "x = tf.keras.layers.Dropout(0.2)(x)   # specifies dropout regularization to reduce chance of overfitting\n",
        "outputs = tf.keras.layers.Dense(5, activation=\"softmax\")(x) # adds the classifier heads that will assign input to one of the five categories\n",
        "\n",
        "model = tf.keras.Model([input_ids, padding_mask, segment_ids], outputs)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "34kK3qB1COJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tf.convert_to_tensor(x_train, dtype=tf.string)\n",
        "x_test  = tf.convert_to_tensor(x_test, dtype=tf.string)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "y_test  = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "# Convert raw strings into numeric token IDs + masks\n",
        "x_train_tokens = bert_preprocess(x_train)  # returns dict of tensors\n",
        "x_test_tokens  = bert_preprocess(x_test)\n",
        "y_train_int = np.argmax(y_train, axis=1)\n",
        "y_test_int  = np.argmax(y_test, axis=1)"
      ],
      "metadata": {
        "id": "qvfbx8ArCv2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of passes over the training data\n",
        "n_epochs = 10\n",
        "\n",
        "# Stops the model from continuing if not improving\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",\n",
        "                                                      patience = 3,\n",
        "                                                      restore_best_weights = True)\n",
        "\n",
        "# How to train the model\n",
        "model.compile(optimizer = \"adam\",\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "# Actual training call\n",
        "model_fit = model.fit(\n",
        "    [x_train_tokens[\"token_ids\"], x_train_tokens[\"padding_mask\"], x_train_tokens[\"segment_ids\"]],\n",
        "    y_train_int,\n",
        "    validation_data=(\n",
        "        [x_test_tokens[\"token_ids\"], x_test_tokens[\"padding_mask\"], x_test_tokens[\"segment_ids\"]],\n",
        "        y_test_int\n",
        "    ),\n",
        "    epochs=n_epochs,\n",
        "    batch_size=8,\n",
        "    callbacks=[earlystop_callback]\n",
        ")\n"
      ],
      "metadata": {
        "id": "vq1QvQE0C5Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict([x_test_tokens[\"token_ids\"], x_test_tokens[\"padding_mask\"], x_test_tokens[\"segment_ids\"]])\n",
        "y_pred_int = y_pred.argmax(axis=1)\n",
        "\n",
        "print(classification_report(y_test_int, y_pred_int))"
      ],
      "metadata": {
        "id": "yEEgHelYKKcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Predict probabilities on test set\n",
        "y_test_probs = model.predict([\n",
        "    x_test_tokens[\"token_ids\"],\n",
        "    x_test_tokens[\"padding_mask\"],\n",
        "    x_test_tokens[\"segment_ids\"]\n",
        "])\n",
        "# Convert to predicted class integers\n",
        "y_test_pred = np.argmax(y_test_probs, axis=1)\n",
        "\n",
        "# Predict on training set\n",
        "y_train_probs = model.predict([\n",
        "    x_train_tokens[\"token_ids\"],\n",
        "    x_train_tokens[\"padding_mask\"],\n",
        "    x_train_tokens[\"segment_ids\"]\n",
        "])\n",
        "y_train_pred = np.argmax(y_train_probs, axis=1)\n",
        "\n",
        "df_all = pd.DataFrame({\n",
        "    \"text\": np.concatenate([x_train, x_test]),\n",
        "    \"actual\": np.concatenate([y_train_int, y_test_int]),\n",
        "    \"predicted\": np.concatenate([y_train_pred, y_test_pred])\n",
        "})\n"
      ],
      "metadata": {
        "id": "7bBiGHbCKlP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_names = [\"tech\", \"business\", \"sport\", \"entertainment\", \"politics\"]\n",
        "df_all[\"actual_name\"] = df_all[\"actual\"].map(lambda x: class_names[x])\n",
        "df_all[\"predicted_name\"] = df_all[\"predicted\"].map(lambda x: class_names[x])\n",
        "\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "s3An3h_XMDRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ChatGPT"
      ],
      "metadata": {
        "id": "jYskrtdLM_iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"text\"].iloc[0])\n"
      ],
      "metadata": {
        "id": "d7e500ABNBn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Import libraries\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "# Enter API key securely\n",
        "openai.api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n"
      ],
      "metadata": {
        "id": "RM600jKjOgHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"respond with one word. Is the following text about entertainment, tech, politics, sport, or business? text: wales want rugby league training wales could follow england s lead by training with a rugby league club.  england have already had a three-day session with leeds rhinos  and wales are thought to be interested in a similar clinic with rivals st helens. saints coach ian millward has given his approval  but if it does happen it is unlikely to be this season. saints have a week s training in portugal next week  while wales will play england in the opening six nations match on 5 february.  we have had an approach from wales   confirmed a saints spokesman.  it s in the very early stages but it is something we are giving serious consideration to.  st helens  who are proud of their welsh connections  are obvious partners for the welsh rugby union  despite a spat in 2001 over the collapse of kieron cunningham s proposed Â£500 000 move to union side swansea. a similar cross-code deal that took iestyn harris from leeds to cardiff in 2001 did go through  before the talented stand-off returned to the 13-man code with bradford bulls. kel coslett  who famously moved from wales to league in the 1960s  is currently saints  football manager  while clive griffiths - wales  defensive coach - is a former st helens player and is thought to be the man behind the latest initiative. scott gibbs  the former wales and lions centre  played for st helens from 1994-96 and was in the challenge cup-winning team at wembley in 1996.\"}\n",
        "    ],\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "zvUjAJhJPv-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed prompt to prepend\n",
        "fixed_prompt = \"respond with one word. Is the following text about entertainment, tech, politics, sport, or business? text: \"\n",
        "\n",
        "# Loop through the first 10 rows and submit to OpenAI\n",
        "for i, text in enumerate(df['text'][:10]):\n",
        "    prompt = fixed_prompt + text\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    summary = response.choices[0].message.content\n",
        "    print(f\"Row {i}:\")\n",
        "    print(f\"Input: {text}\")\n",
        "    print(f\"Output: {summary}\")\n",
        "    print(\"-----\")"
      ],
      "metadata": {
        "id": "wUf2vy9vZc8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"label_text\"].iloc[0:10])\n"
      ],
      "metadata": {
        "id": "FfRIBUWLZrHO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}